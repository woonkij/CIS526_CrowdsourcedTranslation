Project Topic: Crowdsourced Machine Translation
1. Problem Definition
For most statistical machine translation problems, one of the first difficult tasks that the researchers face is gathering the appropriate data. For instance, the parallel language corpus between two languages such as English and French, German, Italian, etc are relatively easy to gather; however, for rare languages, constructing the dataset is very costly and time-consuming.The idea of crowdsourcing translation is introduced to reduce the cost that may incur while employing professional translators; instead, the general idea is to hire many non-professional translators and use their data to start statistical machine translation research. However, it is really difficult to know the quality of non-professionals' translated sentences before manually checking it. Again, manually checking each translated sentence from non-professionals are costly too. 

The basic goal of this assignment is to choose a single sentence among non-professionally translated sentences that is most similar to the reference sentences generated by professional translators. During the grading procedure, the student's choice would be evaluated using re-implemented BLEU score metric. 

2. References
Ambati, Vogel, and Carbonell. "Active Learning and Crowd-Sourcing for Machine Translation"
Snow, O'Connor, Jurafsky, and Ng. "Cheap and Fast--But Is It Good? Evaluating Non-Expert Annotations for Natural Language Tasks"
Omar Zaidan and Chris Callison-Burch. "Crowdsourcing Translation: Professional Quality from Non-Professionals." 2011. In Proceedings ACL-2011.

3. Objective (Scoring) Function:
The student is asked to choose the most likely sentence among four non-professionally translated sentences. For each source sentence, there will be a 'solution sentence' chosen by the student. Using the scoring metric (smoothed-BLEU), student's solution would evaluated with each of the four reference sentences available to grader. Then the final score of the student for a particular source sentence would be assigned after taking an average of these four smoothed-BLEU score. Student's final score would be determined after taking an average of the smoothed-BLEU score for each source sentence. Thus, the student's score will be represented in 0.0 to 1.0 scale (theoretically), with student with higher score having done better.

4. Default System:
The default system is implemented in the simplest fashion. Out of four non-professionally translated sentences, the default system chooses the first sentence and report that sentence to be the most likely translation given a source sentence. The reason for choosing this method is to consistently return the output. 

5. Explanations on Data
Mainly, there are three data files:
- translations.tsv
- survey.tsv
- postedited_translations.tsv
- train_postedited_translations.tsv
- train_translations.tsv
- test_translations.tsv

	- translations.tsv: this file contains the source sentence, four LDC-translated (professional) sentences, four Turk-translated (non-professional) sentences, and four worker identification string which match each non-professional sentence with each worker identification string.

	- survey.tsv: this file contains information on Turk-translators (non-professional translators) in the following categories: native English speaker, native Urdu speaker, current residency in India, current residency in Pakistan, years of speaking English, and years of speaking Urdu.

	- postedited_translations.tsv: this file contains the post-edited versions of the translations, and the ranking judgement about their quality. The file is edited by Turkers who are currently residing in US (so as to have better results; the edits were made in spellings, typos, awkward word choices and grammar). For each source sentence, there are total 10 post-edited non-professionally translated sentences along with the editor's worker identification string.



For the assignment, survey.tsv and postedited_translations.tsv will be available to students as supplementary files that may be used to generate features for Machine Learning. For translations.tsv file, it will be pre-processed prior to distributing the file to the students.
	A. Among 1792 sentences, only first 20% (first 358 sentences) of the original file will be provided to students as a training set, which also include four reference sentences translated by LDCs. It is suggested that the students use these training set to generate plausible / possible features that may increase their 'solution sentence' choice.
	
	B. There will be a testing file, which contains the entire 1792 sentences without any reference sentences. Specifically, this file will contain the following features: source (Urdu) sentence, Turk translation1, Turk translation2, Turk translation3, Turk translation4, WorkerID_1, WorkerID_2, WorkerID_3, WorkerID_4. 

	C. Based on the testing file described in part B, the students are required to make predictions for the most probable candidate sentence.


6. Baseline Implementation
# 

7. Extensions
	A. Extension1 : Machine Learning based approach using AdaBoost Regression.
	- Worker-based Features (binary feature for each Turk worker, total 51)
	- Worker-based Features (binary feature for native English, native Urdu, currently living in India, currently living in Pakistan)
	- Worker-based Features (numeric feature for the number of years speaking English and Urdu)
	- For each feature, there were ambiguous / weird / 'empty' data. For instance, one of the worker reported that he has been speaking English and Urdu for 100 years.
	- For such werid data and empty data (marked with 'UNKNOWN') in the number of years of speaking English and Urdu, such values have been replaced by the sample mean (from other 40-50 workers).
	- For 'UNKNOWN' response in reporting whether a Turker is native in English or Urdu, I have assigned a value of 0.5
	- Sentence-level feature randomly generated: the proportion of length of candidate (sentence translated by Turker) sentence divided by the length of original Urdu sentence.
	- Another sentence-level feature: length of unique words found in candidate sentence by Turker (1 out of 4) divided by the length of unique words found in all four candidate sentences.
	- The reasoning behind this method:
	

	B. Extension2 :	
	
		  
