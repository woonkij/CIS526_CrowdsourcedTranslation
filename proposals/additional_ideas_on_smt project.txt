Original dataset already has professionally-produced reference translations which allowous to objectivelyl and quantitatively compare theq uality of professional and non-professional translations. (Where is it?)


** LDC translations seem professional
** Turk translations generated by non-professional 'turkers' from amazon

Goal of this research is to propose and evaluate such quality control mechanisms for non-professionally translated sentences.

There are 1,792 Urdu sentences from a variety of news and online resources; the set includes four different reference translations for each source sentences, produced by rofessional translation agencies. NIST contracted the LDC (Language Data Consortium) to oversee the translation process and perform quality control.

Sources: 4 original + 10 (somehow edited) sentences

Features used:
SentenceLevel = 6, worker-level = 12
1. Language Model (5-gram) on English Gigaword Corpus
2. Sentence-Length; ratio of the source sentence length against the candidate sentence lengths
3. Web n-gram match percentage: using Google N-Gram Database
4. Web n-gram geometric avg (n=3, 4, 5)
5. Edit rate to other translations (compute avg edit rate distance)

worker-level = 12
1. aggregate feature
2. language abilities (native or not)
3. worker location

ranking = 3 features
1. avg rank: the avg of the five rank labels provided for this translation
2. is-best percentage: how often the translation was top-ranked among the four translations
3. is-better percentage: how often the translation was judged as the better translation, over all pairwise comparisons extracted from the ranks


Or goal: choose the sentences that would increase your BLEU score?
And sort the students' score according to the avg of the BLEU score
