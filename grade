#!/usr/bin/env python
import optparse
import sys
import smoothedBLEU

optparser = optparse.OptionParser()
optparser.add_option("-r", "--readTranslationFiles", dest="input", default="data-others/translations.tsv", help="Read Translation Files.")
optparser.add_option("-o", "--readStudentGeneratedFile", dest="generated_txt", default= "output.txt", help="Read solutiuons generated by students.")

(opts, _) = optparser.parse_args()

# Open the data file, and start parsing
file = open(opts.input)
txt = file.readlines()

# Due to the format of the data, preprocessing is done to generate 
# Reference Dictionary (each source sentence has 4 reference sentences)
# Candidate Dictionary (each source sentence has 4 candidate sentences) 
# Source Dictionary (original Urdu sentence)
# Worker Dictionary (individual non-professional's id assigned from Amazon. Possibly used in ML extensions)
referenceDict = {}
candidateDict = {}
sourceDict = {}
workerDict = {}

for i in xrange(1, len(txt)):
    sentenceList = txt[i].split("\t")
    sourceDict[i] = sentenceList[1] #Urdu
    referenceDict[i] = sentenceList[2:6] #Professionally translated LDC sentences
    candidateDict[i] = sentenceList[6:10] #Non-professionally translated sentences
    workerDict[i] = sentenceList[11:] #Individual worker id

answerList = [sourceDict, referenceDict, candidateDict, workerDict]

# Grading Starts
# Opens student-generated file: "output.txt"

# file = open(opts.generated_txt)
generated_output = sys.stdin.readlines()


cumScore = 0

# For each output generated by student, smoothed-BLEU score is calculated against 
# each of four reference sentences provided by LDCs. 
# Then, the average of these four smoothed-BLEU score is returned as a BLEU score for each source sentence.
# The average of the entire BLEU score is returned as the student's grade
# The student with highest returned BLEU score wins.
for idx, line in enumerate(generated_output):
    score = 0.0
    n = 0
    for ref in referenceDict[idx + 1]:
        try:
            score += smoothedBLEU.smoothedBLEU(line, ref)
            n += 1
        except: pass

    try:
        avg_score = score / n
        cumScore += avg_score
    except: pass

finalScore = cumScore / len(generated_output)
print finalScore