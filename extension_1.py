from __future__ import division
import numpy as np
import math
from collections import Counter
import os
from sklearn.ensemble import AdaBoostRegressor

# ****************************************************************************************************
# * smoothedBLEU method used to evaluate sentence-level quality of each sentence generated by Turkers
# * referred to the paper : #http://acl2014.org/acl2014/w14-33/pdf/w14-3346.pdf
# ****************************************************************************************************
def smoothedBLEU(hypothesis, reference):
    updateVar = 1.0
    K = 5.0

    ngram_Weight_Vector = [0 for _ in xrange(4)]

    #SMOOTHING_4

    h_ngramList = [0 for _ in xrange(4)]
    for n in xrange(1, 5): #1,2,3,4
        h_ngrams = Counter([tuple(hypothesis[i:i + n]) for i in xrange(len(hypothesis) + 1 - n)])
        r_ngrams = Counter([tuple(reference[i:i + n]) for i in xrange(len(reference) + 1 - n)])

        num_match = max([sum((h_ngrams & r_ngrams).values()), 0])

        #number of tokens for translation phrase
        h_ngramList[n - 1] = sum(h_ngrams.values())

        if num_match == 0:
            #print math.log(len(hypothesis))
            updateVar = updateVar * K / (math.log(len(hypothesis)))
            ngram_Weight_Vector[n - 1] = float(1.0 / updateVar)
        else:
            ngram_Weight_Vector[n - 1] = num_match

    #SMOOTHING_5
    new_weight_vec = [0 for _ in xrange(5)]
    new_weight_vec[0] = ngram_Weight_Vector[0] + 1
    ngram_Weight_Vector.append(0) #for updating purpose: last = 0

    for i in xrange(1, 5): #1,2,3,4
        new_weight_vec[i] = (new_weight_vec[i - 1] + ngram_Weight_Vector[i - 1] + ngram_Weight_Vector[i]) / 3

    #pop initial dummy value
    new_weight_vec.remove(new_weight_vec[0])
    score = 0
    for i in xrange(4):
        #print h_ngramList
        score = score + 0.25*math.log(new_weight_vec[i] / h_ngramList[i])
    score = math.exp(score)
    BrevityPenalty = min(1, math.exp(1 - len(reference)/len(hypothesis)))
    score = score * BrevityPenalty
    return score


# ****************************************************************************************************
# * Here, the idea was to use Machine Learning to identify the Turk-translated sentence which is most similar
# * to the translations generated by LDC (professionals).
# * I have used sentence-level BLEU Score to first measure the quality of each Turk-generated sentence
# ****************************************************************************************************
def constructBLEUDict(refDict, hypDict):
    bleuDict = {}
    main_keys = refDict.keys()

    refLen = len(refDict.keys())
    hypLen = len(hypDict.keys())

    num_hyp = len(hypDict[1])

    Y = np.empty([refLen * num_hyp, 1], dtype='f')

    bleu = [0.0, 0.0, 0.0, 0.0]
    for idx in xrange(1, len(hypDict) + 1):
        for refIdx in xrange(4):
            for hypIdx in xrange(4):
                #For the case where there was no sentence generated by the Turk, I have assigned the score of 0.
                if hypDict[idx][hypIdx] == 'n/a':
                    bleu[hypIdx] = bleu[hypIdx] + 0
                else:
                    bleu[hypIdx] = bleu[hypIdx] + smoothedBLEU(hypDict[idx][hypIdx], refDict[idx][refIdx])

        bleu = [float(item / 4) for item in bleu]

        Y[4 * (idx - 1) , 0] = bleu[0]
        Y[4 * (idx - 1) + 1, 0] = bleu[1]
        Y[4 * (idx - 1) + 2, 0] = bleu[2]
        Y[4 * (idx - 1) + 3, 0] = bleu[3]

    #Return the Y-label (estimated Smoothed-BLEU score on sentence level)
    return Y

# ****************************************************************************************************
# * Reads the data and generate source dictionary, reference dictionary, candidate dictionary and worker dictionary
# * This preprocessing provides neat data that can be easily used in feature generation and training procedure
# ****************************************************************************************************
def parseFile(path):
    file = open(path)
    txt = file.readlines()

    header = txt[0]

    referenceDict = {}
    candidateDict = {}
    sourceDict = {}
    workerDict = {}


    for i in xrange(1, len(txt)):
        sentenceList = txt[i].split("\t")
        sourceDict[i] = sentenceList[1] #URDU
        referenceDict[i] = sentenceList[2:6]
        candidateDict[i] = sentenceList[6:10]
        workerDict[i] = sentenceList[10:]

    answerList = [sourceDict, referenceDict, candidateDict, workerDict]
    return answerList


# ****************************************************************************************************
# * I have mostly used the features from the surveys.tsv file which contained explicit information on
# * the quality / reliance of each Turker. I have used the following features and modified inconsistent data
# * from the surveys.tsv to create features with reasonable variance
# * Binary: (0, 1) depending on whether the Turker is Native English, Native Urdu, Lives in India, Lives in Pakistan.
# * Number of years speaking English, Number of years speaking Urdu
# * And, additional 51 features (for each Turker; another 0/1 binary variable to imply which Turker generated which
# * sentences.
# * Also, within the data, I have replaced 'Unknown' value to 0.5 (when the Turker did not fully identify himself as
# * native English or Urdu.)
# * For 'Unknown' value for the number of years of speaking English and Urdu, I have calculated the mean of other Turker's
# * data, and replaced missing value with the mean of the data group.
# * For one of the Turker (which stated that he has been speaking English and Urdu for 100 years, which is a noise), I have
# * updated his years of speaking English and Urdu value with the mean from other sample, too.
# ****************************************************************************************************
def constructWorkerFeature(path):
    file = open(path)
    txt = file.readlines()

    header = txt[0].split()

    #number of translators = 51
    #thus, the features are 51 + 6(native eng, native urdu, location india, location paki, yrsEnglish, yrsUrdu)
    #return the dictionary regarding the information of the worker
    workerFeatureDict = {}
    EnglishYrCtr = 0
    UrduYrCtr = 0

    for i in xrange(1, len(txt)):
        row = txt[i].split()
        id = row[0]

        #Process of encoding YES into 1, while NO into 0
        row = [1 if item =='YES' else item for item in row]
        row = [0 if item == 'NO' else item for item in row]

        #Calculating Mean value for #ofYrs Speaking English and Urdu
        if row[5] == 'UNKNOWN':
            pass
        else:
            EnglishYrCtr += int(row[5])

        if row[6] == 'UNKNOWN':
            pass
        else:
            UrduYrCtr += int(row[6])

        workerFeatureDict[id] = row

    EnglishYrCtr = EnglishYrCtr / (len(txt) - 1)
    UrduYrCtr = UrduYrCtr / (len(txt) - 1)



    #Manually fixing obviously wrong value
    workerFeatureDict['a2iouac3vzbks6'][5] = int(EnglishYrCtr)
    workerFeatureDict['a2iouac3vzbks6'][6] = int(UrduYrCtr)

    #DATA PREPROCESSING
    #UPDATE two unknown values to 0.5 for whether that person is native in English or Urdu
    #UPDATE UNKNOWN yrs of speaking language with mean value
    for key in workerFeatureDict.keys():
        if workerFeatureDict[key][1] == 'UNKNOWN':
            workerFeatureDict[key][1] = 0.5

        if workerFeatureDict[key][2] == 'UNKNOWN':
            workerFeatureDict[key][2] = 0.5

        if workerFeatureDict[key][5] == 'UNKNOWN':
            workerFeatureDict[key][5] = int(EnglishYrCtr)

        if workerFeatureDict[key][6] == 'UNKNOWN':
            workerFeatureDict[key][6] = int(UrduYrCtr)

    return workerFeatureDict

# ****************************************************************************************************
# * I did implement Dynamic Programming-based way of counting the edit distances, however, I have not
# * implemented this in feature generation process.
# ****************************************************************************************************
def edDistRecursiveMemo(x, y, memo=None):
    ''' A version of edDistRecursive with memoization.  For each x, y we see, we
        record result from edDistRecursiveMemo(x, y).  In the future, we retrieve
        recorded result rather than re-run the function. '''
    if memo is None: memo = {}
    if len(x) == 0:
        return len(y)
    if len(y) == 0:
        return len(x)
    if (len(x), len(y)) in memo:
        return memo[(len(x), len(y))]
    delt = 1 if x[-1] != y[-1] else 0
    diag = edDistRecursiveMemo(x[:-1], y[:-1], memo) + delt
    vert = edDistRecursiveMemo(x[:-1], y, memo) + 1
    horz = edDistRecursiveMemo(x, y[:-1], memo) + 1
    ans = min(diag, vert, horz)
    memo[(len(x), len(y))] = ans
    return ans


def parsePostEdited(filepath):
    file = open(filepath)
    txt = file.readlines()

    header = txt[0].split("\t")
    for item in txt:
        item = item.split("\t")
    print header


# ****************************************************************************************************
# * Read test file and generate appropriate format for Machine-Learning purpose.
# ****************************************************************************************************
def parseTestFile(filepath):
    file = open(filepath)
    txt = file.readlines()
    header = txt[0].split()

    sourceDict = {}
    candidateDict = {}
    workerDict = {}

    for i in xrange(1, len(txt)):
        row = txt[i].split("\t")
        sourceDict[i] = row[1] #Urdu
        candidateDict[i] = row[2:6]
        workerDict[i] = row[6:]

    answerList = [sourceDict, candidateDict, workerDict]
    return answerList


# ****************************************************************************************************
# * Here, both the formatting of data are done so that actual ML algorithm can be applied.
# * I have used AdaBoost Regression (Boosted Learning) to first try to 'model' smoothed-BLEU score for each
# * sentence generated by Turkers. Then, I have designed the algorithm to choose the sentence with highest
# * smoothed-BLEU score out of 4 which represent the same source sentence.
# ****************************************************************************************************
def trainAndformatML(Y_train, train_refDict, train_candidateDict, train_workerDict, workerFeatureDict, sourceDict, testcandidateDict, testworkerDict):
    num_examples = Y_train.size

    #worker_id - key dictionary to easily index the feature construction.
    workerID = {}
    ctr = 1
    for key in workerFeatureDict.keys():
        workerID[key] = ctr
        ctr += 1

    print "Start Train"
    X_train = np.empty([num_examples, 59], dtype='f') #58 features; 51 writer, 7 other

    #Construct Train Feature
    for i in xrange(1, len(train_workerDict.keys()) + 1):
        totalSet = set()
        list = train_candidateDict[i]
        for str in list:
            for token in str.split():
                totalSet.add(token)

        for j in xrange(4):
            X_train[4 * (i - 1) + j, 0] = len(train_candidateDict[i][j]) / len(sourceDict[i])  #length proportional to source

            target_set = set(train_candidateDict[i][j].split())
            X_train[4 * (i - 1) + j, 1] = len(target_set.intersection(totalSet)) / len(totalSet) # check # of unique elements in each sentence
            worker_id = train_workerDict[i][j].rstrip()

            if worker_id == 'n/a':
                for x in xrange(1, 7):
                    X_train[4 * (i - 1) + j, x + 1] = 0


            else:
                X_train[4 * (i - 1) + j, 2] = workerFeatureDict[worker_id][1]   #Native English? 0/1
                X_train[4 * (i - 1) + j, 3] = workerFeatureDict[worker_id][2]   #Native Urdu? 0/1
                X_train[4 * (i - 1) + j, 4] = workerFeatureDict[worker_id][3]   #Live in India? 0/1
                X_train[4 * (i - 1) + j, 5] = workerFeatureDict[worker_id][4]   #Live in Pakistan? 0/1
                X_train[4 * (i - 1) + j, 6] = workerFeatureDict[worker_id][5]   #Year Speaking English?
                X_train[4 * (i - 1) + j, 7] = workerFeatureDict[worker_id][6]   #Year Speaking Urdu?

            #Worker ID assignment
            try:
                update_idx = workerID[worker_id]
                X_train[4 * (i - 1) + j, 7 + update_idx] = 1

            except KeyError:
                pass

    print ("Start Adaboost Training")
    Y_train = np.ravel(Y_train)
    clf = AdaBoostRegressor(n_estimators=1000, loss='exponential')
    clf.fit(X_train, Y_train)

    #NEED TO TEST
    #RE-FORMAT THE TEST DATA
    X_test = np.empty([4 * len(sourceDict), 59], dtype='f')

    for i in xrange(1, len(sourceDict.keys()) + 1):
        totalSet = set()
        list = testcandidateDict[i]
        for str in list:
            for token in str.split():
                totalSet.add(token)

        for j in xrange(4):
            X_test[4 * (i - 1) + j, 0] = len(testcandidateDict[i][j]) / len(sourceDict[i]) # length proportional to sentence length

            target_set = set(testcandidateDict[i][j].split())
            X_test[4 * (i - 1) + j, 1] = len(target_set.intersection(totalSet)) / len(totalSet) #check # of unique elements
            worker_id = testworkerDict[i][j].rstrip()

            if worker_id == 'n/a':
                for x in xrange(1, 7):
                    X_test[4 * (i - 1) + j, x + 1] = 0

            else:
                X_test[4 * (i - 1) + j, 2] = workerFeatureDict[worker_id][1]   #Native English? 0/1
                X_test[4 * (i - 1) + j, 3] = workerFeatureDict[worker_id][2]   #Native Urdu? 0/1
                X_test[4 * (i - 1) + j, 4] = workerFeatureDict[worker_id][3]   #Live in India? 0/1
                X_test[4 * (i - 1) + j, 5] = workerFeatureDict[worker_id][4]   #Live in Pakistan? 0/1
                X_test[4 * (i - 1) + j, 6] = workerFeatureDict[worker_id][5]   #Year Speaking English?
                X_test[4 * (i - 1) + j, 7] = workerFeatureDict[worker_id][6]   #Year Speaking Urdu?

            #Worker ID Assignment
            try:
                update_idx = workerID[worker_id]
                X_test[4 * (i - 1) + j, 7 + update_idx] = 1

            except KeyError:
                pass

    #START TESTING
    print "Testing"
    Y_predicted = clf.predict(X_test)

    solutionList = [0 for _ in xrange(len(sourceDict))]

    print len(solutionList)

    for i in xrange(len(sourceDict)):
        maxVal = max(Y_predicted[4 * i : 4 * (i + 1)])
        maxIdx = [idx for idx, val in enumerate(Y_predicted[4 * i : 4 * (i + 1)]) if val == maxVal]
        solutionList[i] = (maxVal, maxIdx[0] + 4 * i)


    output = open("extension_output.txt", "a")
    for i in xrange(1, len(solutionList) + 1):
        idx = solutionList[i-1][1] % 4
        output.write((testcandidateDict[i][idx]) + '\n')

    output.close()




if __name__ == "__main__":
    print "Start"
    filepath = os.path.relpath('data-train/train_translations.tsv') #actual data
    parsedDataList = parseFile(filepath)

    train_refDict = parsedDataList[1]
    train_candidateDict = parsedDataList[2]
    train_workerDict = parsedDataList[3]

    #CONSTRUCT BLEUDICT for train purpose
    Y_train = constructBLEUDict(train_refDict, train_candidateDict)

    #WORKER FEATURE
    filepath = os.path.relpath('data-train/survey.tsv') #worker feature
    workerFeatureDict = constructWorkerFeature(filepath)

    #TEST DICT
    filepath = os.path.relpath('data-test/test_translations.tsv')
    parsedTestData = parseTestFile(filepath)
    entire_sourceDict = parsedTestData[0]
    test_candidateDict = parsedTestData[1]
    test_workerDict = parsedTestData[2]
    #[sourceDict, candidateDict, workerDict]

    print "Train and Test"
    trainAndformatML(Y_train, train_refDict, train_candidateDict, train_workerDict, workerFeatureDict, entire_sourceDict, test_candidateDict, test_workerDict)

    print "Complete"
