#!/usr/bin/env python
from __future__ import division
import optparse
import sys
import numpy as np
import math
from collections import Counter
import smoothedBLEU
import os
from sklearn.ensemble import AdaBoostRegressor

optparser = optparse.OptionParser()
optparser.add_option("-r", "--readTranslationFiles", dest="input", default="data-train/train_translations.tsv",
                     help="Read Translation Files.")
optparser.add_option("-s", "--readSurvey", dest='survey', default='data-train/survey.tsv')
optparser.add_option("-t", "--readTestFile", dest="test", default='data-test/test_translations.tsv', help="Read Test File.")
optparser.add_option("-d", "--baselineData", dest='data', default=None, help="Data for learning algorithms")
(opts, _) = optparser.parse_args()



#parseFile
file = open(opts.input)
txt = file.readlines()
header = txt[0]

referenceDict = {}
candidateDict = {}
sourceDict = {}
workerDict = {}


for i in xrange(1, len(txt)):
    sentenceList = txt[i].split("\t")
    sourceDict[i] = sentenceList[1] #URDU
    referenceDict[i] = sentenceList[2:6]
    candidateDict[i] = sentenceList[6:10]
    workerDict[i] = sentenceList[10:]

parsedDataList = [sourceDict, referenceDict, candidateDict, workerDict]

train_refDict = parsedDataList[1]
train_candidateDict = parsedDataList[2]
train_workerDict = parsedDataList[3]


#constructBLEUDict
refDict = train_refDict
hypDict = train_candidateDict
bleuDict = {}
main_keys = refDict.keys()

refLen = len(refDict.keys())
hypLen = len(hypDict.keys())

num_hyp = len(hypDict[1])

Y = np.empty([refLen * num_hyp, 1], dtype='f')

bleu = [0.0, 0.0, 0.0, 0.0]
for idx in xrange(1, len(hypDict) + 1):
    for refIdx in xrange(4):
        for hypIdx in xrange(4):
            #For the case where there was no sentence generated by the Turk, I have assigned the score of 0.
            if hypDict[idx][hypIdx] == 'n/a':
                bleu[hypIdx] = bleu[hypIdx] + 0
            else:
                bleu[hypIdx] = bleu[hypIdx] + smoothedBLEU.smoothedBLEU(hypDict[idx][hypIdx], refDict[idx][refIdx])

    bleu = [float(item / 4) for item in bleu]

    Y[4 * (idx - 1) , 0] = bleu[0]
    Y[4 * (idx - 1) + 1, 0] = bleu[1]
    Y[4 * (idx - 1) + 2, 0] = bleu[2]
    Y[4 * (idx - 1) + 3, 0] = bleu[3]

#Return the Y-label (estimated Smoothed-BLEU score on sentence level)
#return Y

#constructWorkerFeature
# workerFeatureDict = constructWorkerFeature(filepath)
file = open(opts.survey)
txt = file.readlines()

#number of translators = 51
#thus, the features are 51 + 6(native eng, native urdu, location india, location paki, yrsEnglish, yrsUrdu)
#return the dictionary regarding the information of the worker
workerFeatureDict = {}
EnglishYrCtr = 0
UrduYrCtr = 0

for i in xrange(1, len(txt)):
    row = txt[i].split()
    id = row[0]

    #Process of encoding YES into 1, while NO into 0
    row = [1 if item =='YES' else item for item in row]
    row = [0 if item == 'NO' else item for item in row]

    #Calculating Mean value for #ofYrs Speaking English and Urdu
    if row[5] == 'UNKNOWN':
        pass
    else:
        EnglishYrCtr += int(row[5])

    if row[6] == 'UNKNOWN':
        pass
    else:
        UrduYrCtr += int(row[6])

    workerFeatureDict[id] = row

EnglishYrCtr = EnglishYrCtr / (len(txt) - 1)
UrduYrCtr = UrduYrCtr / (len(txt) - 1)



#Manually fixing obviously wrong value
workerFeatureDict['a2iouac3vzbks6'][5] = int(EnglishYrCtr)
workerFeatureDict['a2iouac3vzbks6'][6] = int(UrduYrCtr)

#DATA PREPROCESSING
#UPDATE two unknown values to 0.5 for whether that person is native in English or Urdu
#UPDATE UNKNOWN yrs of speaking language with mean value
for key in workerFeatureDict.keys():
    if workerFeatureDict[key][1] == 'UNKNOWN':
        workerFeatureDict[key][1] = 0.5

    if workerFeatureDict[key][2] == 'UNKNOWN':
        workerFeatureDict[key][2] = 0.5

    if workerFeatureDict[key][5] == 'UNKNOWN':
        workerFeatureDict[key][5] = int(EnglishYrCtr)

    if workerFeatureDict[key][6] == 'UNKNOWN':
        workerFeatureDict[key][6] = int(UrduYrCtr)

#return workerFeatureDict

##TEST
#parseTestFile
file = open(opts.test)
txt = file.readlines()
header = txt[0].split()

sourceDict = {}
candidateDict = {}
workerDict = {}

for i in xrange(1, len(txt)):
    row = txt[i].split("\t")
    sourceDict[i] = row[1] #Urdu
    candidateDict[i] = row[2:6]
    workerDict[i] = row[6:]

parsedTestData = [sourceDict, candidateDict, workerDict]


entire_sourceDict = parsedTestData[0]
test_candidateDict = parsedTestData[1]
test_workerDict = parsedTestData[2]


#TEST
Y_train = Y
sourceDict = entire_sourceDict
testcandidateDict = test_candidateDict
testworkerDict = test_workerDict    

num_examples = Y_train.size

#worker_id - key dictionary to easily index the feature construction.
workerID = {}
ctr = 1
for key in workerFeatureDict.keys():
    workerID[key] = ctr
    ctr += 1

sys.stderr.write("Start Train\n")
X_train = np.empty([num_examples, 59], dtype='f') #58 features; 51 writer, 7 other

#Construct Train Feature
for i in xrange(1, len(train_workerDict.keys()) + 1):
    totalSet = set()
    list = train_candidateDict[i]
    for str in list:
        for token in str.split():
            totalSet.add(token)

    for j in xrange(4):
        X_train[4 * (i - 1) + j, 0] = len(train_candidateDict[i][j]) / len(sourceDict[i])  #length proportional to source

        target_set = set(train_candidateDict[i][j].split())
        X_train[4 * (i - 1) + j, 1] = len(target_set.intersection(totalSet)) / len(totalSet) # check # of unique elements in each sentence
        worker_id = train_workerDict[i][j].rstrip()

        if worker_id == 'n/a':
            for x in xrange(1, 7):
                X_train[4 * (i - 1) + j, x + 1] = 0


        else:
            X_train[4 * (i - 1) + j, 2] = workerFeatureDict[worker_id][1]   #Native English? 0/1
            X_train[4 * (i - 1) + j, 3] = workerFeatureDict[worker_id][2]   #Native Urdu? 0/1
            X_train[4 * (i - 1) + j, 4] = workerFeatureDict[worker_id][3]   #Live in India? 0/1
            X_train[4 * (i - 1) + j, 5] = workerFeatureDict[worker_id][4]   #Live in Pakistan? 0/1
            X_train[4 * (i - 1) + j, 6] = workerFeatureDict[worker_id][5]   #Year Speaking English?
            X_train[4 * (i - 1) + j, 7] = workerFeatureDict[worker_id][6]   #Year Speaking Urdu?

        #Worker ID assignment
        try:
            update_idx = workerID[worker_id]
            X_train[4 * (i - 1) + j, 7 + update_idx] = 1

        except KeyError:
            pass

sys.stderr.write("Start Adaboost Training\n")
Y_train = np.ravel(Y_train)
clf = AdaBoostRegressor(n_estimators=500, loss='exponential')
clf.fit(X_train, Y_train)

#NEED TO TEST
#RE-FORMAT THE TEST DATA
X_test = np.empty([4 * len(sourceDict), 59], dtype='f')

for i in xrange(1, len(sourceDict.keys()) + 1):
    totalSet = set()
    list = testcandidateDict[i]
    for str in list:
        for token in str.split():
            totalSet.add(token)

    for j in xrange(4):
        X_test[4 * (i - 1) + j, 0] = len(testcandidateDict[i][j]) / len(sourceDict[i]) # length proportional to sentence length

        target_set = set(testcandidateDict[i][j].split())
        X_test[4 * (i - 1) + j, 1] = len(target_set.intersection(totalSet)) / len(totalSet) #check # of unique elements
        worker_id = testworkerDict[i][j].rstrip()

        if worker_id == 'n/a':
            for x in xrange(1, 7):
                X_test[4 * (i - 1) + j, x + 1] = 0

        else:
            X_test[4 * (i - 1) + j, 2] = workerFeatureDict[worker_id][1]   #Native English? 0/1
            X_test[4 * (i - 1) + j, 3] = workerFeatureDict[worker_id][2]   #Native Urdu? 0/1
            X_test[4 * (i - 1) + j, 4] = workerFeatureDict[worker_id][3]   #Live in India? 0/1
            X_test[4 * (i - 1) + j, 5] = workerFeatureDict[worker_id][4]   #Live in Pakistan? 0/1
            X_test[4 * (i - 1) + j, 6] = workerFeatureDict[worker_id][5]   #Year Speaking English?
            X_test[4 * (i - 1) + j, 7] = workerFeatureDict[worker_id][6]   #Year Speaking Urdu?

        #Worker ID Assignment
        try:
            update_idx = workerID[worker_id]
            X_test[4 * (i - 1) + j, 7 + update_idx] = 1

        except KeyError:
            pass

#START TESTING
sys.stderr.write("Testing\n")
Y_predicted = clf.predict(X_test)

solutionList = [0 for _ in xrange(len(sourceDict))]


for i in xrange(len(sourceDict)):
    maxVal = max(Y_predicted[4 * i : 4 * (i + 1)])
    maxIdx = [idx for idx, val in enumerate(Y_predicted[4 * i : 4 * (i + 1)]) if val == maxVal]
    solutionList[i] = (maxVal, maxIdx[0] + 4 * i)


#Write to output file
if opts.data is None:
    for i in xrange(1, len(solutionList) + 1):
        idx = solutionList[i-1][1] % 4
        sys.stdout.write(testcandidateDict[i][idx] + '\n')

